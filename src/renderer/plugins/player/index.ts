interface HTMLAudioElementChrome extends HTMLAudioElement {
  setSinkId: (id: string) => Promise<void>
}

// åŒAudioå…ƒç´ ç³»ç»Ÿ - ç”¨äºæ— ç¼åˆ‡æ¢
let audioA: HTMLAudioElementChrome | null = null
let audioB: HTMLAudioElementChrome | null = null
let currentAudioId: 'A' | 'B' = 'A'  // å½“å‰æ’­æ”¾çš„audio
let audio: HTMLAudioElementChrome | null = null  // æŒ‡å‘å½“å‰æ´»è·ƒçš„audio

let audioContext: AudioContext
let mediaSourceA: MediaElementAudioSourceNode | null = null
let mediaSourceB: MediaElementAudioSourceNode | null = null
let mediaSource: MediaElementAudioSourceNode  // æŒ‡å‘å½“å‰æ´»è·ƒçš„mediaSource
let crossfadeTimer: NodeJS.Timeout | null = null  // äº¤å‰æ·¡å…¥æ·¡å‡ºå®šæ—¶å™¨
let hasTriggeredNearEnd: boolean = false  // æ ‡è®°æ˜¯å¦å·²è§¦å‘å³å°†ç»“æŸäº‹ä»¶ï¼ˆé¿å…é‡å¤ï¼‰
let nearEndCallback: (() => void) | null = null  // å³å°†ç»“æŸçš„å›è°ƒ

/**
 * æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿›è¡Œäº¤å‰æ·¡å…¥æ·¡å‡º
 */
export const isCrossfading = (): boolean => {
  return crossfadeTimer !== null
}
let analyser: AnalyserNode
// https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext
// https://benzleung.gitbooks.io/web-audio-api-mini-guide/content/chapter5-1.html
export const freqs = [31, 62, 125, 250, 500, 1000, 2000, 4000, 8000, 16000] as const
type Freqs = (typeof freqs)[number]
let biquads: Map<`hz${Freqs}`, BiquadFilterNode>
export const freqsPreset = [
  {
    name: 'pop',
    hz31: 6,
    hz62: 5,
    hz125: -3,
    hz250: -2,
    hz500: 5,
    hz1000: 4,
    hz2000: -4,
    hz4000: -3,
    hz8000: 6,
    hz16000: 4,
  },
  {
    name: 'dance',
    hz31: 4,
    hz62: 3,
    hz125: -4,
    hz250: -6,
    hz500: 0,
    hz1000: 0,
    hz2000: 3,
    hz4000: 4,
    hz8000: 4,
    hz16000: 5,
  },
  {
    name: 'rock',
    hz31: 7,
    hz62: 6,
    hz125: 2,
    hz250: 1,
    hz500: -3,
    hz1000: -4,
    hz2000: 2,
    hz4000: 1,
    hz8000: 4,
    hz16000: 5,
  },
  {
    name: 'classical',
    hz31: 6,
    hz62: 7,
    hz125: 1,
    hz250: 2,
    hz500: -1,
    hz1000: 1,
    hz2000: -4,
    hz4000: -6,
    hz8000: -7,
    hz16000: -8,
  },
  {
    name: 'vocal',
    hz31: -5,
    hz62: -6,
    hz125: -4,
    hz250: -3,
    hz500: 3,
    hz1000: 4,
    hz2000: 5,
    hz4000: 4,
    hz8000: -3,
    hz16000: -3,
  },
  {
    name: 'slow',
    hz31: 5,
    hz62: 4,
    hz125: 2,
    hz250: 0,
    hz500: -2,
    hz1000: 0,
    hz2000: 3,
    hz4000: 6,
    hz8000: 7,
    hz16000: 8,
  },
  {
    name: 'electronic',
    hz31: 6,
    hz62: 5,
    hz125: 0,
    hz250: -5,
    hz500: -4,
    hz1000: 0,
    hz2000: 6,
    hz4000: 8,
    hz8000: 8,
    hz16000: 7,
  },
  {
    name: 'subwoofer',
    hz31: 8,
    hz62: 7,
    hz125: 5,
    hz250: 4,
    hz500: 0,
    hz1000: 0,
    hz2000: 0,
    hz4000: 0,
    hz8000: 0,
    hz16000: 0,
  },
  {
    name: 'soft',
    hz31: -5,
    hz62: -5,
    hz125: -4,
    hz250: -4,
    hz500: 3,
    hz1000: 2,
    hz2000: 4,
    hz4000: 4,
    hz8000: 0,
    hz16000: 0,
  },
] as const
export const convolutions = [
  { name: 'telephone', mainGain: 0.0, sendGain: 3.0, source: 'filter-telephone.wav' }, // ç”µè¯
  { name: 's2_r4_bd', mainGain: 1.8, sendGain: 0.9, source: 's2_r4_bd.wav' }, // æ•™å ‚
  { name: 'bright_hall', mainGain: 0.8, sendGain: 2.4, source: 'bright-hall.wav' },
  { name: 'cinema_diningroom', mainGain: 0.6, sendGain: 2.3, source: 'cinema-diningroom.wav' },
  {
    name: 'dining_living_true_stereo',
    mainGain: 0.6,
    sendGain: 1.8,
    source: 'dining-living-true-stereo.wav',
  },
  {
    name: 'living_bedroom_leveled',
    mainGain: 0.6,
    sendGain: 2.1,
    source: 'living-bedroom-leveled.wav',
  },
  { name: 'spreader50_65ms', mainGain: 1, sendGain: 2.5, source: 'spreader50-65ms.wav' },
  // { name: 'spreader25_125ms', mainGain: 1, sendGain: 2.5, source: 'spreader25-125ms.wav' },
  // { name: 'backslap', mainGain: 1.8, sendGain: 0.8, source: 'backslap1.wav' },
  { name: 's3_r1_bd', mainGain: 1.8, sendGain: 0.8, source: 's3_r1_bd.wav' },
  { name: 'matrix_1', mainGain: 1.5, sendGain: 0.9, source: 'matrix-reverb1.wav' },
  { name: 'matrix_2', mainGain: 1.3, sendGain: 1, source: 'matrix-reverb2.wav' },
  {
    name: 'cardiod_35_10_spread',
    mainGain: 1.8,
    sendGain: 0.6,
    source: 'cardiod-35-10-spread.wav',
  },
  {
    name: 'tim_omni_35_10_magnetic',
    mainGain: 1,
    sendGain: 0.2,
    source: 'tim-omni-35-10-magnetic.wav',
  },
  // { name: 'spatialized', mainGain: 1.8, sendGain: 0.8, source: 'spatialized8.wav' },
  // { name: 'zing_long_stereo', mainGain: 0.8, sendGain: 1.8, source: 'zing-long-stereo.wav' },
  { name: 'feedback_spring', mainGain: 1.8, sendGain: 0.8, source: 'feedback-spring.wav' },
  // { name: 'tim_omni_rear_blend', mainGain: 1.8, sendGain: 0.8, source: 'tim-omni-rear-blend.wav' },
] as const
// åŠéŸ³
// export const semitones = [-1.5, -1, -0.5, 0.5, 1, 1.5, 2, 2.5, 3, 3.5] as const

let convolver: ConvolverNode
let convolverSourceGainNode: GainNode
let convolverOutputGainNode: GainNode
let convolverDynamicsCompressor: DynamicsCompressorNode
let gainNode: GainNode
let panner: PannerNode
let pitchShifterNode: AudioWorkletNode
let pitchShifterNodePitchFactor: AudioParam | null
let pitchShifterNodeLoadStatus: 'none' | 'loading' | 'unconnect' | 'connected' = 'none'
let pitchShifterNodeTempValue = 1
let defaultChannelCount = 2
export const soundR = 0.5

export const createAudio = () => {
  if (audio) return

  // åˆ›å»ºä¸¤ä¸ªaudioå…ƒç´ ç”¨äºæ— ç¼åˆ‡æ¢
  audioA = new window.Audio() as HTMLAudioElementChrome
  audioA.controls = false
  audioA.autoplay = true
  audioA.preload = 'auto'
  audioA.crossOrigin = 'anonymous'

  audioB = new window.Audio() as HTMLAudioElementChrome
  audioB.controls = false
  audioB.autoplay = false  // Båˆå§‹ä¸è‡ªåŠ¨æ’­æ”¾
  audioB.preload = 'auto'
  audioB.crossOrigin = 'anonymous'

  // é»˜è®¤ä½¿ç”¨ audioA
  audio = audioA
  currentAudioId = 'A'

  // CRITICAL FIX V5: æ·»åŠ  timeupdate ç›‘å¬å™¨ï¼Œæå‰è§¦å‘åˆ‡æ¢
  // åœ¨éŸ³é¢‘å³å°†ç»“æŸå‰ 1 ç§’è§¦å‘å›è°ƒï¼Œå®ç°çœŸæ­£çš„é‡å æ’­æ”¾
  const handleTimeupdateA = () => {
    if (currentAudioId === 'A' && audioA && !hasTriggeredNearEnd && nearEndCallback) {
      const remaining = audioA.duration - audioA.currentTime
      if (remaining > 0 && remaining <= 1) {
        console.log(`â° AudioA near end (${remaining.toFixed(2)}s remaining), triggering early crossfade`)
        hasTriggeredNearEnd = true
        nearEndCallback()
      }
    }
  }

  const handleTimeupdateB = () => {
    if (currentAudioId === 'B' && audioB && !hasTriggeredNearEnd && nearEndCallback) {
      const remaining = audioB.duration - audioB.currentTime
      if (remaining > 0 && remaining <= 1) {
        console.log(`â° AudioB near end (${remaining.toFixed(2)}s remaining), triggering early crossfade`)
        hasTriggeredNearEnd = true
        nearEndCallback()
      }
    }
  }

  audioA.addEventListener('timeupdate', handleTimeupdateA)
  audioB.addEventListener('timeupdate', handleTimeupdateB)
}

const initAnalyser = () => {
  analyser = audioContext.createAnalyser()
  analyser.fftSize = 256
}

const initBiquadFilter = () => {
  biquads = new Map()
  let i

  for (const item of freqs) {
    const filter = audioContext.createBiquadFilter()
    biquads.set(`hz${item}`, filter)
    filter.type = 'peaking'
    filter.frequency.value = item
    filter.Q.value = 1.4
    filter.gain.value = 0
  }

  for (i = 1; i < freqs.length; i++) {
    biquads.get(`hz${freqs[i - 1]}`)!.connect(biquads.get(`hz${freqs[i]}`)!)
  }
}

const initConvolver = () => {
  convolverSourceGainNode = audioContext.createGain()
  convolverOutputGainNode = audioContext.createGain()
  convolverDynamicsCompressor = audioContext.createDynamicsCompressor()
  convolver = audioContext.createConvolver()
  convolver.connect(convolverOutputGainNode)
  convolverSourceGainNode.connect(convolverDynamicsCompressor)
  convolverOutputGainNode.connect(convolverDynamicsCompressor)
}

const initPanner = () => {
  panner = audioContext.createPanner()
}

const initGain = () => {
  gainNode = audioContext.createGain()
}

const initAdvancedAudioFeatures = () => {
  if (audioContext) return
  if (!audio || !audioA || !audioB) throw new Error('audio not defined')
  audioContext = new window.AudioContext({ latencyHint: 'playback' })
  defaultChannelCount = audioContext.destination.channelCount

  initAnalyser()
  initBiquadFilter()
  initConvolver()
  initPanner()
  initGain()

  // åˆ›å»ºåŒaudioçš„mediaSource
  // source -> analyser -> biquadFilter -> pitchShifter -> [(convolver & convolverSource)->convolverDynamicsCompressor] -> panner -> gain
  mediaSourceA = audioContext.createMediaElementSource(audioA)
  mediaSourceB = audioContext.createMediaElementSource(audioB)

  // CRITICAL FIX: åŒæ—¶è¿æ¥ä¸¤ä¸ª mediaSource åˆ° analyser
  // è¿™æ ·åˆ‡æ¢æ—¶ä¸éœ€è¦é‡æ–°è¿æ¥ï¼Œé¿å…éŸ³é¢‘æµä¸­æ–­è§¦å‘ onWaiting
  mediaSourceA.connect(analyser)
  mediaSourceB.connect(analyser)

  // é»˜è®¤ä½¿ç”¨ mediaSourceAï¼ˆé€šè¿‡ audio å¼•ç”¨æ§åˆ¶ï¼Œä¸¤ä¸ª source éƒ½å·²è¿æ¥ï¼‰
  mediaSource = mediaSourceA

  analyser.connect(biquads.get(`hz${freqs[0]}`)!)
  const lastBiquadFilter = biquads.get(`hz${freqs.at(-1)!}`)!
  lastBiquadFilter.connect(convolverSourceGainNode)
  lastBiquadFilter.connect(convolver)
  convolverDynamicsCompressor.connect(panner)
  panner.connect(gainNode)
  gainNode.connect(audioContext.destination)

  // éŸ³é¢‘è¾“å‡ºè®¾å¤‡æ”¹å˜æ—¶åˆ·æ–° audio node è¿æ¥
  window.app_event.on('playerDeviceChanged', handleMediaListChange)

  // audio.addEventListener('playing', connectAudioNode)
  // audio.addEventListener('pause', disconnectAudioNode)
  // audio.addEventListener('waiting', disconnectAudioNode)
  // audio.addEventListener('emptied', disconnectAudioNode)
  // if (!audio.paused) connectAudioNode()
}

const handleMediaListChange = () => {
  // CRITICAL FIX: é‡æ–°è¿æ¥ä¸¤ä¸ª mediaSourceï¼ˆè®¾å¤‡æ”¹å˜æ—¶ï¼‰
  if (mediaSourceA) {
    mediaSourceA.disconnect()
    mediaSourceA.connect(analyser)
  }
  if (mediaSourceB) {
    mediaSourceB.disconnect()
    mediaSourceB.connect(analyser)
  }
}

// let isConnected = true
// const connectAudioNode = () => {
//   if (isConnected) return
//   console.log('connect Node')
//   mediaSource.connect(analyser)
//   isConnected = true
//   if (pitchShifterNodeTempValue == 1 && pitchShifterNodeLoadStatus == 'connected') {
//     disconnectPitchShifterNode()
//   }
// }

// const disconnectAudioNode = () => {
//   if (!isConnected) return
//   console.log('disconnect Node')
//   mediaSource.disconnect()
//   isConnected = false
//   if (pitchShifterNodeTempValue == 1 && pitchShifterNodeLoadStatus == 'connected') {
//     disconnectPitchShifterNode()
//   }
// }

export const getAudioContext = () => {
  initAdvancedAudioFeatures()
  return audioContext
}

let unsubMediaListChangeEvent: (() => void) | null = null
export const setMaxOutputChannelCount = (enable: boolean) => {
  if (enable) {
    initAdvancedAudioFeatures()
    audioContext.destination.channelCountMode = 'max'
    audioContext.destination.channelCount = audioContext.destination.maxChannelCount
    // navigator.mediaDevices.addEventListener('devicechange', handleMediaListChange)
    if (!unsubMediaListChangeEvent) {
      let handleMediaListChange = () => {
        setMaxOutputChannelCount(true)
      }
      window.app_event.on('playerDeviceChanged', handleMediaListChange)
      unsubMediaListChangeEvent = () => {
        window.app_event.off('playerDeviceChanged', handleMediaListChange)
        unsubMediaListChangeEvent = null
      }
    }
  } else {
    unsubMediaListChangeEvent?.()
    if (audioContext && audioContext.destination.channelCountMode != 'explicit') {
      audioContext.destination.channelCount = defaultChannelCount
      // audioContext.destination.channelInterpretation
      audioContext.destination.channelCountMode = 'explicit'
    }
  }
}

export const getAnalyser = (): AnalyserNode | null => {
  initAdvancedAudioFeatures()
  return analyser
}

export const getBiquadFilter = () => {
  initAdvancedAudioFeatures()
  return biquads
}

// let isConvolverConnected = false
export const setConvolver = (buffer: AudioBuffer | null, mainGain: number, sendGain: number) => {
  initAdvancedAudioFeatures()
  convolver.buffer = buffer
  // console.log(mainGain, sendGain)
  if (buffer) {
    convolverSourceGainNode.gain.value = mainGain
    convolverOutputGainNode.gain.value = sendGain
  } else {
    convolverSourceGainNode.gain.value = 1
    convolverOutputGainNode.gain.value = 0
  }
}

export const setConvolverMainGain = (gain: number) => {
  if (convolverSourceGainNode.gain.value == gain) return
  // console.log(gain)
  convolverSourceGainNode.gain.value = gain
}

export const setConvolverSendGain = (gain: number) => {
  if (convolverOutputGainNode.gain.value == gain) return
  // console.log(gain)
  convolverOutputGainNode.gain.value = gain
}

let pannerInfo = {
  x: 0,
  y: 0,
  z: 0,
  soundR: 0.5,
  rad: 0,
  speed: 1,
  intv: null as NodeJS.Timeout | null,
}
const setPannerXYZ = (nx: number, ny: number, nz: number) => {
  pannerInfo.x = nx
  pannerInfo.y = ny
  pannerInfo.z = nz
  // console.log(pannerInfo)
  panner.positionX.value = nx * pannerInfo.soundR
  panner.positionY.value = ny * pannerInfo.soundR
  panner.positionZ.value = nz * pannerInfo.soundR
}
export const setPannerSoundR = (r: number) => {
  pannerInfo.soundR = r
}

export const setPannerSpeed = (speed: number) => {
  pannerInfo.speed = speed
  if (pannerInfo.intv) startPanner()
}
export const stopPanner = () => {
  if (pannerInfo.intv) {
    clearInterval(pannerInfo.intv)
    pannerInfo.intv = null
    pannerInfo.rad = 0
  }
  panner.positionX.value = 0
  panner.positionY.value = 0
  panner.positionZ.value = 0
}

export const startPanner = () => {
  initAdvancedAudioFeatures()
  if (pannerInfo.intv) {
    clearInterval(pannerInfo.intv)
    pannerInfo.intv = null
    pannerInfo.rad = 0
  }
  pannerInfo.intv = setInterval(() => {
    pannerInfo.rad += 1
    if (pannerInfo.rad > 360) pannerInfo.rad -= 360
    setPannerXYZ(
      Math.sin((pannerInfo.rad * Math.PI) / 180),
      Math.cos((pannerInfo.rad * Math.PI) / 180),
      Math.cos((pannerInfo.rad * Math.PI) / 180)
    )
  }, pannerInfo.speed * 10)
}

let isConnected = true
const connectNode = () => {
  if (isConnected) return
  console.log('connect Node')
  analyser?.connect(biquads.get(`hz${freqs[0]}`)!)
  isConnected = true
  if (pitchShifterNodeTempValue == 1 && pitchShifterNodeLoadStatus == 'connected') {
    disconnectPitchShifterNode()
  }
}
const disconnectNode = () => {
  if (!isConnected) return
  console.log('disconnect Node')
  analyser?.disconnect()
  isConnected = false
  if (pitchShifterNodeTempValue == 1 && pitchShifterNodeLoadStatus == 'connected') {
    disconnectPitchShifterNode()
  }
}
const connectPitchShifterNode = () => {
  console.log('connect Pitch Shifter Node')
  audio!.addEventListener('playing', connectNode)
  audio!.addEventListener('pause', disconnectNode)
  audio!.addEventListener('waiting', disconnectNode)
  audio!.addEventListener('emptied', disconnectNode)
  if (audio!.paused) disconnectNode()

  const lastBiquadFilter = biquads.get(`hz${freqs.at(-1)!}`)!
  lastBiquadFilter.disconnect()
  lastBiquadFilter.connect(pitchShifterNode)

  pitchShifterNode.connect(convolver)
  pitchShifterNode.connect(convolverSourceGainNode)
  // convolverDynamicsCompressor.disconnect(panner)
  // convolverDynamicsCompressor.connect(pitchShifterNode)
  // pitchShifterNode.connect(panner)
  pitchShifterNodeLoadStatus = 'connected'
  pitchShifterNodePitchFactor!.value = pitchShifterNodeTempValue
}
const disconnectPitchShifterNode = () => {
  console.log('disconnect Pitch Shifter Node')
  const lastBiquadFilter = biquads.get(`hz${freqs.at(-1)!}`)!
  lastBiquadFilter.disconnect()
  lastBiquadFilter.connect(convolver)
  lastBiquadFilter.connect(convolverSourceGainNode)
  pitchShifterNodeLoadStatus = 'unconnect'
  pitchShifterNodePitchFactor = null

  audio!.removeEventListener('playing', connectNode)
  audio!.removeEventListener('pause', disconnectNode)
  audio!.removeEventListener('waiting', disconnectNode)
  audio!.removeEventListener('emptied', disconnectNode)
  connectNode()
}
const loadPitchShifterNode = () => {
  pitchShifterNodeLoadStatus = 'loading'
  initAdvancedAudioFeatures()
  // source -> analyser -> biquadFilter -> audioWorklet(pitch shifter) -> [(convolver & convolverSource)->convolverDynamicsCompressor] -> panner -> gain
  void audioContext.audioWorklet
    .addModule(
      new URL(
        /* webpackChunkName: 'pitch_shifter.audioWorklet' */
        './pitch-shifter/phase-vocoder.js',
        import.meta.url
      )
    )
    .then(() => {
      console.log('pitch shifter audio worklet loaded')
      // https://github.com/olvb/phaze/issues/26#issuecomment-1574629971
      pitchShifterNode = new AudioWorkletNode(audioContext, 'phase-vocoder-processor', {
        outputChannelCount: [2],
      })
      let pitchFactorParam = pitchShifterNode.parameters.get('pitchFactor')
      if (!pitchFactorParam) return
      pitchShifterNodePitchFactor = pitchFactorParam
      pitchShifterNodeLoadStatus = 'unconnect'
      if (pitchShifterNodeTempValue == 1) return

      connectPitchShifterNode()
    })
}

export const setPitchShifter = (val: number) => {
  // console.log('setPitchShifter', val)
  pitchShifterNodeTempValue = val
  switch (pitchShifterNodeLoadStatus) {
    case 'loading':
      break
    case 'none':
      loadPitchShifterNode()
      break
    case 'connected':
      // a: 1 = åŠéŸ³
      // value = 2 ** (a / 12)
      pitchShifterNodePitchFactor!.value = val
      break
    case 'unconnect':
      connectPitchShifterNode()
      break
  }
}

export const hasInitedAdvancedAudioFeatures = (): boolean => audioContext != null

// ============ åŒAudioæ— ç¼åˆ‡æ¢ç³»ç»Ÿ ============

/**
 * è·å–ä¸‹ä¸€ä¸ªå¾…ç”¨çš„audioå…ƒç´  (ç”¨äºé¢„åŠ è½½)
 */
export const getNextAudio = (): HTMLAudioElementChrome | null => {
  return currentAudioId === 'A' ? audioB : audioA
}

/**
 * è·å–å½“å‰æ´»è·ƒçš„audio ID
 */
export const getCurrentAudioId = (): 'A' | 'B' => {
  return currentAudioId
}

/**
 * é¢„åŠ è½½ä¸‹ä¸€é¦–æ­Œæ›²çš„URL (åŒAudioæ— ç¼åˆ‡æ¢æ ¸å¿ƒ)
 * @param src éŸ³é¢‘URL
 * @returns Promise<void> å½“éŸ³é¢‘å‡†å¤‡å¥½æ’­æ”¾æ—¶ resolve
 */
export const preloadNextMusic = (src: string): Promise<void> => {
  const nextAudio = getNextAudio()
  if (!nextAudio) return Promise.resolve()

  const nextAudioId = currentAudioId === 'A' ? 'B' : 'A'
  console.log(`Preloading next music to audio${nextAudioId}`)

  // å…³é”®ä¿®å¤ï¼šç¡®ä¿é¢„åŠ è½½æ—¶ä¸ä¼šè‡ªåŠ¨æ’­æ”¾
  nextAudio.autoplay = false
  nextAudio.preload = 'auto'  // å¼ºåˆ¶é¢„åŠ è½½ç­–ç•¥
  nextAudio.src = src

  return new Promise<void>((resolve, reject) => {
    const timeout = setTimeout(() => {
      cleanup()
      console.warn(`âš ï¸ Preload timeout after 10s (readyState=${nextAudio.readyState})`)
      // è¶…æ—¶ä¹Ÿ resolveï¼Œä½†ä¼šåœ¨åˆ‡æ¢æ—¶å¯èƒ½ä»æœ‰å»¶è¿Ÿ
      resolve()
    }, 10000)  // å»¶é•¿åˆ° 10 ç§’

    let hasResolved = false

    const cleanup = () => {
      if (hasResolved) return
      hasResolved = true
      clearTimeout(timeout)
      nextAudio.removeEventListener('canplay', onCanPlay)
      nextAudio.removeEventListener('error', onError)
      nextAudio.removeEventListener('loadeddata', onLoadedData)
    }

    // å…³é”®ä¿®å¤ï¼šç›‘å¬ canplay äº‹ä»¶ï¼ˆä¸æ˜¯è½®è¯¢ readyStateï¼‰
    const onCanPlay = () => {
      console.log(`âœ… Audio ${nextAudioId} canplay (readyState=${nextAudio.readyState})`)

      // å†æ¬¡ç¡®è®¤ readyState >= 3
      if (nextAudio.readyState >= 3) {
        cleanup()
        resolve()
      } else {
        console.log(`âš ï¸ canplay but readyState=${nextAudio.readyState}, waiting...`)
      }
    }

    // ä½œä¸ºå¤‡é€‰ï¼Œç›‘å¬ loadeddata äº‹ä»¶
    const onLoadedData = () => {
      console.log(`ğŸ“¦ Audio ${nextAudioId} loadeddata (readyState=${nextAudio.readyState})`)
      if (nextAudio.readyState >= 2) {  // HAVE_CURRENT_DATA ä»¥ä¸Š
        // å»¶è¿Ÿä¸€ç‚¹å†æ£€æŸ¥ï¼Œç¡®ä¿è§£ç å®Œæˆ
        setTimeout(() => {
          if (nextAudio.readyState >= 3) {
            cleanup()
            resolve()
          }
        }, 100)
      }
    }

    const onError = (e: Event) => {
      cleanup()
      console.error(`âŒ Preload error for audio${nextAudioId}:`, e)
      reject(new Error('Preload failed'))
    }

    nextAudio.addEventListener('canplay', onCanPlay, { once: false })
    nextAudio.addEventListener('loadeddata', onLoadedData, { once: false })
    nextAudio.addEventListener('error', onError, { once: true })

    // å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶è§¦å‘åŠ è½½ï¼ˆçŸ­æš‚é™éŸ³æ’­æ”¾ï¼‰
    nextAudio.volume = 0  // é™éŸ³
    nextAudio.muted = true  // åŒä¿é™©é™éŸ³

    // å…ˆ load()
    nextAudio.load()

    // å»¶è¿Ÿ 100ms åçŸ­æš‚æ’­æ”¾ï¼Œå¼ºåˆ¶æµè§ˆå™¨å¼€å§‹è§£ç 
    setTimeout(() => {
      console.log(`ğŸ”Š Forcing audio${nextAudioId} to load by brief silent play...`)
      const playPromise = nextAudio.play()

      if (playPromise) {
        playPromise.then(() => {
          console.log(`âœ… Audio${nextAudioId} playing silently (readyState=${nextAudio.readyState})`)

          // CRITICAL FIX V2: æ’­æ”¾çŸ­æš‚æ—¶é—´åç«‹å³æš‚åœå¹¶é‡ç½®åˆ° 0
          // ç›®çš„ï¼šå¼ºåˆ¶æµè§ˆå™¨è§£ç ï¼Œä½†ä¿æŒ currentTime = 0ï¼Œé¿å…åˆ‡æ¢æ—¶ seeking
          setTimeout(() => {
            if (nextAudio.src) {
              nextAudio.pause()
              nextAudio.currentTime = 0  // seeking åœ¨æš‚åœçŠ¶æ€ä¸‹å®Œæˆï¼Œä¸å½±å“åˆ‡æ¢
              nextAudio.muted = false
              console.log(`ğŸ¯ Audio${nextAudioId} preloaded and ready at currentTime=0 (paused, readyState=${nextAudio.readyState})`)
            }
          }, 200)  // æ’­æ”¾ 200ms è¶³ä»¥è®©æµè§ˆå™¨è§£ç å¹¶ç¼“å­˜æ•°æ®
        }).catch(err => {
          console.warn(`âš ï¸ Brief play failed (expected in some browsers):`, err)
          nextAudio.muted = false
        })
      }
    }, 100)
  })
}

/**
 * åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªaudio (æ— ç¼åˆ‡æ¢æ ¸å¿ƒå‡½æ•° - ä½¿ç”¨äº¤å‰æ·¡å…¥æ·¡å‡º)
 * @returns æ˜¯å¦åˆ‡æ¢æˆåŠŸ
 */
export const switchToNextAudio = (): boolean => {
  const nextAudio = getNextAudio()
  const nextAudioId = currentAudioId === 'A' ? 'B' : 'A'

  console.log(`ğŸ”„ Attempting to switch to audio${nextAudioId}`)
  console.log(`   Next audio exists: ${!!nextAudio}`)
  console.log(`   Next audio src: ${nextAudio?.src ? nextAudio.src.substring(0, 50) + '...' : 'EMPTY'}`)

  if (!nextAudio || !nextAudio.src) {
    console.warn(`âŒ Next audio not ready for switch (audio${nextAudioId})`)
    return false
  }

  // CRITICAL FIX V5: é‡ç½® near end æ ‡å¿—ï¼Œå…è®¸æ–°æ­Œæ›²è§¦å‘æå‰åˆ‡æ¢
  hasTriggeredNearEnd = false
  console.log(`ğŸ”„ Reset hasTriggeredNearEnd for audio${nextAudioId}`)

  const previousAudio = audio
  const previousAudioId = currentAudioId

  // ä¿å­˜å½“å‰éŸ³é‡è®¾ç½®ï¼ˆç”¨äºæ·¡å…¥æ·¡å‡ºï¼‰
  const targetVolume = previousAudio?.volume ?? 1
  const wasMuted = previousAudio?.muted ?? false

  // CRITICAL FIX: ä¸¤ä¸ª mediaSource å·²åœ¨åˆå§‹åŒ–æ—¶è¿æ¥åˆ° analyser
  // åˆ‡æ¢æ—¶åªéœ€æ›´æ–° mediaSource å¼•ç”¨ï¼Œæ— éœ€ connect/disconnect
  let previousMediaSource: MediaElementAudioSourceNode | null = null
  if (audioContext && mediaSourceA && mediaSourceB) {
    previousMediaSource = mediaSource
    mediaSource = currentAudioId === 'A' ? mediaSourceB : mediaSourceA
    console.log(`ğŸ”Š Switched AudioContext to media source ${nextAudioId}`)
  }

  // ç»§æ‰¿å…¶ä»–éŸ³é¢‘å±æ€§
  if (previousAudio) {
    nextAudio.playbackRate = previousAudio.playbackRate
    nextAudio.defaultPlaybackRate = previousAudio.defaultPlaybackRate
    nextAudio.preservesPitch = previousAudio.preservesPitch
  }

  // CRITICAL FIX: ä¸‹ä¸€ä¸ªaudioä»é™éŸ³å¼€å§‹æ’­æ”¾
  nextAudio.volume = 0
  nextAudio.muted = false  // ä¸ä½¿ç”¨muteï¼Œè€Œæ˜¯ç”¨volume=0å®ç°é™éŸ³
  nextAudio.autoplay = true

  // åˆ‡æ¢audioå¼•ç”¨ï¼ˆåœ¨æ’­æ”¾å‰åˆ‡æ¢ï¼‰
  audio = nextAudio
  currentAudioId = nextAudioId

  console.log(`âœ… Switched to audio${currentAudioId}, starting crossfade`)

  // CRITICAL FIX V5: nextAudio åº”è¯¥æ˜¯æš‚åœçŠ¶æ€ï¼ˆpaused, currentTime=0, readyState>=3ï¼‰
  console.log(`   Next audio status: paused=${nextAudio.paused}, currentTime=${nextAudio.currentTime.toFixed(3)}s, readyState=${nextAudio.readyState}`)

  // éªŒè¯é¢„åŠ è½½çŠ¶æ€ï¼šcurrentTime åº”è¯¥å·²ç»æ˜¯ 0
  if (nextAudio.currentTime > 0.05) {
    console.warn(`âš ï¸ Next audio currentTime not at 0 (${nextAudio.currentTime.toFixed(3)}s), resetting...`)
    nextAudio.currentTime = 0
  }

  // CRITICAL FIX V5: å…ˆè®© nextAudio å¼€å§‹é™éŸ³æ’­æ”¾ï¼Œé¿å… play() å¯åŠ¨å»¶è¿Ÿ
  // è¿™æ · crossfade åªè°ƒæ•´éŸ³é‡ï¼Œä¸ä¼šè§¦å‘ onWaiting
  nextAudio.volume = 0
  nextAudio.muted = false

  // å¯åŠ¨äº¤å‰æ·¡å…¥æ·¡å‡ºçš„å‡½æ•°
  const startCrossfade = () => {
    console.log(`â–¶ï¸ Starting crossfade for audio${nextAudioId}`)

    // CRITICAL FIX: å»¶é•¿äº¤å‰æ·¡å…¥æ·¡å‡ºä»¥è¦†ç›–å¯èƒ½çš„ onWaiting ç¼“å†²
    const fadeDuration = 500  // å»¶é•¿åˆ° 500ms è¦†ç›–ç¼“å†²å»¶è¿Ÿ
    const fadeSteps = 25      // å¢åŠ åˆ° 25 æ­¥æ›´å¹³æ»‘
    const fadeInterval = fadeDuration / fadeSteps

    // æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§å®šæ—¶å™¨
    if (crossfadeTimer) {
      clearInterval(crossfadeTimer)
      crossfadeTimer = null
    }

    let step = 0
    crossfadeTimer = setInterval(() => {
      step++
      const progress = step / fadeSteps

      // ä½¿ç”¨ ease-in-out æ›²çº¿ä»£æ›¿çº¿æ€§æ·¡å…¥æ·¡å‡º
      const easedProgress = progress < 0.5
        ? 2 * progress * progress
        : 1 - Math.pow(-2 * progress + 2, 2) / 2

      // ä¸‹ä¸€ä¸ªaudioæ·¡å…¥ï¼ˆ0 â†’ targetVolumeï¼‰
      if (nextAudio) {
        nextAudio.volume = easedProgress * targetVolume
      }

      // å½“å‰audioæ·¡å‡ºï¼ˆtargetVolume â†’ 0ï¼‰
      if (previousAudio && !previousAudio.paused) {
        previousAudio.volume = (1 - easedProgress) * targetVolume
      }

      if (step >= fadeSteps) {
        clearInterval(crossfadeTimer!)
        crossfadeTimer = null  // æ¸…ç©ºå¼•ç”¨

        // ç¡®ä¿æœ€ç»ˆéŸ³é‡ç²¾ç¡®å¹¶æ¢å¤mutedçŠ¶æ€
        if (nextAudio) {
          nextAudio.volume = targetVolume
          nextAudio.muted = wasMuted
        }

        // æ·¡å…¥æ·¡å‡ºå®Œæˆåï¼Œæ¸…ç†æ—§audio
        if (previousAudio) {
          previousAudio.pause()
          previousAudio.autoplay = false
          try {
            previousAudio.currentTime = 0
          } catch {}
          previousAudio.src = ''
          previousAudio.removeAttribute('src')
          previousAudio.volume = targetVolume  // æ¢å¤éŸ³é‡ä¸ºä¸‹æ¬¡ä½¿ç”¨
          previousAudio.muted = false

          // Note: previousMediaSource ä¿æŒè¿æ¥çŠ¶æ€ï¼ˆåˆå§‹åŒ–æ—¶å·²è¿æ¥ï¼‰
          // ä¸éœ€è¦ disconnectï¼Œä¸¤ä¸ª mediaSource å§‹ç»ˆè¿æ¥åˆ° analyser
          console.log(`ğŸ§¹ Cleaned up audio${previousAudioId} after crossfade`)
        }

        console.log(`âœ… Crossfade completed: audio${previousAudioId} â†’ audio${nextAudioId}`)
      }
    }, fadeInterval)
  }

  // CRITICAL FIX V2: nextAudio åº”è¯¥æ˜¯æš‚åœçŠ¶æ€ï¼Œå…ˆæ’­æ”¾å†å¼€å§‹ crossfade
  // è¿™æ ·é¿å…äº† seeking æ“ä½œï¼ˆcurrentTime å·²ç»æ˜¯ 0ï¼‰
  console.log(`â–¶ï¸ Starting playback of audio${nextAudioId} from paused state`)
  const playPromise = nextAudio.play()
  if (playPromise && typeof playPromise.catch === 'function') {
    void playPromise.then(() => {
      console.log(`âœ… Audio${nextAudioId} started playing, beginning crossfade`)
      startCrossfade()
    }).catch(err => {
      console.error('âŒ Failed to start next audio for crossfade:', err)
      // æ’­æ”¾å¤±è´¥æ—¶æ¢å¤éŸ³é‡
      if (nextAudio) {
        nextAudio.volume = targetVolume
        nextAudio.muted = wasMuted
      }
    })
  } else {
    // åŒæ­¥ play()ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
    startCrossfade()
  }

  return true
}

/**
 * æ¸…ç©ºä¸‹ä¸€ä¸ªaudioçš„èµ„æº
 */
export const clearNextAudio = () => {
  const nextAudio = getNextAudio()
  if (nextAudio && nextAudio.src) {
    nextAudio.pause()
    nextAudio.autoplay = false  // é‡ç½®autoplay
    nextAudio.src = ''
    nextAudio.removeAttribute('src')
    console.log(`Cleared next audio${currentAudioId === 'A' ? 'B' : 'A'}`)
  }
}

// ============================================

export const setResource = (src: string) => {
  if (audio) {
    audio.src = src
    // é‡ç½®å³å°†ç»“æŸçš„æ ‡å¿—ï¼ˆæ–°æ­Œæ›²å¼€å§‹ï¼‰
    hasTriggeredNearEnd = false
  }
}

/**
 * æ³¨å†Œå³å°†ç»“æŸçš„å›è°ƒï¼ˆåœ¨éŸ³é¢‘å‰©ä½™ 1 ç§’æ—¶è§¦å‘ï¼‰
 */
export const onNearEnd = (callback: () => void) => {
  nearEndCallback = callback
  return () => {
    nearEndCallback = null
  }
}

export const setPlay = () => {
  void audio?.play()
}

export const setPause = () => {
  audio?.pause()
}

export const setStop = () => {
  if (audio) {
    audio.src = ''
    audio.removeAttribute('src')
  }
}

export const isEmpty = (): boolean => !audio?.src

export const setLoopPlay = (isLoop: boolean) => {
  if (audio) audio.loop = isLoop
}

export const getPlaybackRate = (): number => {
  return audio?.defaultPlaybackRate ?? 1
}

export const setPlaybackRate = (rate: number) => {
  if (!audio) return
  audio.defaultPlaybackRate = rate
  audio.playbackRate = rate
}

export const setPreservesPitch = (preservesPitch: boolean) => {
  if (!audio) return
  audio.preservesPitch = preservesPitch
}

export const getMute = (): boolean => {
  return audio?.muted ?? false
}

export const setMute = (isMute: boolean) => {
  if (audio) audio.muted = isMute
}

export const getCurrentTime = () => {
  return audio?.currentTime || 0
}

export const setCurrentTime = (time: number) => {
  if (audio) audio.currentTime = time
}

export const setMediaDeviceId = async (mediaDeviceId: string): Promise<void> => {
  if (!audio) return
  return audio.setSinkId(mediaDeviceId)
}

export const setVolume = (volume: number) => {
  if (audio) audio.volume = volume
}

export const getDuration = () => {
  return audio?.duration || 0
}

// export const getPlaybackRate = () => {
//   return audio?.playbackRate ?? 1
// }

type Noop = () => void

// åŒAudioäº‹ä»¶ç›‘å¬ - åŒæ—¶ç›‘å¬ä¸¤ä¸ªaudioå…ƒç´ 
// ä¼˜åŒ–ä¼ é€’audioIdå‚æ•°ï¼Œè®©å›è°ƒå‡½æ•°èƒ½è¯†åˆ«æ˜¯å“ªä¸ªaudioè§¦å‘çš„äº‹ä»¶
type AudioEventCallback = (audioId?: 'A' | 'B') => void

export const onPlaying = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('playing', handlerA)
  audioB.addEventListener('playing', handlerB)
  return () => {
    audioA?.removeEventListener('playing', handlerA)
    audioB?.removeEventListener('playing', handlerB)
  }
}

export const onPause = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('pause', handlerA)
  audioB.addEventListener('pause', handlerB)
  return () => {
    audioA?.removeEventListener('pause', handlerA)
    audioB?.removeEventListener('pause', handlerB)
  }
}

export const onEnded = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('ended', handlerA)
  audioB.addEventListener('ended', handlerB)
  return () => {
    audioA?.removeEventListener('ended', handlerA)
    audioB?.removeEventListener('ended', handlerB)
  }
}

export const onError = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('error', handlerA)
  audioB.addEventListener('error', handlerB)
  return () => {
    audioA?.removeEventListener('error', handlerA)
    audioB?.removeEventListener('error', handlerB)
  }
}

export const onLoadeddata = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('loadeddata', handlerA)
  audioB.addEventListener('loadeddata', handlerB)
  return () => {
    audioA?.removeEventListener('loadeddata', handlerA)
    audioB?.removeEventListener('loadeddata', handlerB)
  }
}

export const onLoadstart = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('loadstart', handlerA)
  audioB.addEventListener('loadstart', handlerB)
  return () => {
    audioA?.removeEventListener('loadstart', handlerA)
    audioB?.removeEventListener('loadstart', handlerB)
  }
}

export const onCanplay = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('canplay', handlerA)
  audioB.addEventListener('canplay', handlerB)
  return () => {
    audioA?.removeEventListener('canplay', handlerA)
    audioB?.removeEventListener('canplay', handlerB)
  }
}

export const onEmptied = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('emptied', handlerA)
  audioB.addEventListener('emptied', handlerB)
  return () => {
    audioA?.removeEventListener('emptied', handlerA)
    audioB?.removeEventListener('emptied', handlerB)
  }
}

export const onTimeupdate = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('timeupdate', handlerA)
  audioB.addEventListener('timeupdate', handlerB)
  return () => {
    audioA?.removeEventListener('timeupdate', handlerA)
    audioB?.removeEventListener('timeupdate', handlerB)
  }
}

// ç¼“å†²ä¸­
export const onWaiting = (callback: AudioEventCallback) => {
  if (!audioA || !audioB) throw new Error('audio not defined')

  const handlerA = () => callback('A')
  const handlerB = () => callback('B')

  audioA.addEventListener('waiting', handlerA)
  audioB.addEventListener('waiting', handlerB)
  return () => {
    audioA?.removeEventListener('waiting', handlerA)
    audioB?.removeEventListener('waiting', handlerB)
  }
}

// å¯è§æ€§æ”¹å˜
export const onVisibilityChange = (callback: Noop) => {
  document.addEventListener('visibilitychange', callback)
  return () => {
    document.removeEventListener('visibilitychange', callback)
  }
}

export const getErrorCode = () => {
  return audio?.error?.code
}
